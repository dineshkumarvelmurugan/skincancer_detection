{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d603dc06-d3c1-40c4-a1ce-94078099f63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "Classes: ['akiec' 'bcc' 'bkl' 'df' 'mel' 'nv' 'vasc'] \n",
      "#classes: 7  #images: 9701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_634824/1563143256.py:84: UserWarning: Argument(s) 'min_holes, max_holes, min_height, max_height, min_width, max_width, fill_value' are not valid for transform CoarseDropout\n",
      "  A.CoarseDropout(\n",
      "                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/60 | Train Loss: 1.0505 Acc: 0.4008 | Val Loss: 0.6964 Acc: 0.5483 BalAcc: 0.6092 MacroF1: 0.4248 | LR: 0.000200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002/60 | Train Loss: 0.9069 Acc: 0.4850 | Val Loss: 0.5643 Acc: 0.6293 BalAcc: 0.6216 MacroF1: 0.4972 | LR: 0.000199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003/60 | Train Loss: 0.8639 Acc: 0.5097 | Val Loss: 0.6015 Acc: 0.5833 BalAcc: 0.6354 MacroF1: 0.4654 | LR: 0.000199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004/60 | Train Loss: 0.8665 Acc: 0.5346 | Val Loss: 0.4281 Acc: 0.6809 BalAcc: 0.7267 MacroF1: 0.6351 | LR: 0.000200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005/60 | Train Loss: 0.6872 Acc: 0.6226 | Val Loss: 0.3158 Acc: 0.7228 BalAcc: 0.7319 MacroF1: 0.6554 | LR: 0.000199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 006/60 | Train Loss: 0.5945 Acc: 0.6571 | Val Loss: 0.3885 Acc: 0.7358 BalAcc: 0.7323 MacroF1: 0.6776 | LR: 0.000199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007/60 | Train Loss: 0.5723 Acc: 0.6707 | Val Loss: 0.3200 Acc: 0.7578 BalAcc: 0.7515 MacroF1: 0.6939 | LR: 0.000198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 008/60 | Train Loss: 0.5253 Acc: 0.6930 | Val Loss: 0.3522 Acc: 0.6816 BalAcc: 0.7261 MacroF1: 0.6732 | LR: 0.000196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 009/60 | Train Loss: 0.5143 Acc: 0.7006 | Val Loss: 0.3142 Acc: 0.6685 BalAcc: 0.7397 MacroF1: 0.6844 | LR: 0.000195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010/60 | Train Loss: 0.4934 Acc: 0.7077 | Val Loss: 0.2610 Acc: 0.8131 BalAcc: 0.7604 MacroF1: 0.7231 | LR: 0.000193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 011/60 | Train Loss: 0.4514 Acc: 0.7333 | Val Loss: 0.2417 Acc: 0.8038 BalAcc: 0.7370 MacroF1: 0.7374 | LR: 0.000191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 012/60 | Train Loss: 0.4659 Acc: 0.7271 | Val Loss: 0.3657 Acc: 0.7602 BalAcc: 0.7518 MacroF1: 0.7019 | LR: 0.000189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 013/60 | Train Loss: 0.4453 Acc: 0.7345 | Val Loss: 0.3639 Acc: 0.7066 BalAcc: 0.7537 MacroF1: 0.6446 | LR: 0.000186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 014/60 | Train Loss: 0.4650 Acc: 0.7266 | Val Loss: 0.2722 Acc: 0.7400 BalAcc: 0.6944 MacroF1: 0.6988 | LR: 0.000183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 015/60 | Train Loss: 0.4248 Acc: 0.7437 | Val Loss: 0.2555 Acc: 0.7870 BalAcc: 0.7320 MacroF1: 0.7237 | LR: 0.000180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 016/60 | Train Loss: 0.4054 Acc: 0.7553 | Val Loss: 0.2482 Acc: 0.7942 BalAcc: 0.7568 MacroF1: 0.7322 | LR: 0.000177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 017/60 | Train Loss: 0.3914 Acc: 0.7651 | Val Loss: 0.2235 Acc: 0.8344 BalAcc: 0.7667 MacroF1: 0.7605 | LR: 0.000173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 018/60 | Train Loss: 0.3842 Acc: 0.7602 | Val Loss: 0.2452 Acc: 0.8224 BalAcc: 0.7763 MacroF1: 0.7425 | LR: 0.000169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 019/60 | Train Loss: 0.3855 Acc: 0.7606 | Val Loss: 0.2266 Acc: 0.8482 BalAcc: 0.7661 MacroF1: 0.7514 | LR: 0.000165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020/60 | Train Loss: 0.3803 Acc: 0.7686 | Val Loss: 0.1970 Acc: 0.8712 BalAcc: 0.7698 MacroF1: 0.7970 | LR: 0.000161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 021/60 | Train Loss: 0.3651 Acc: 0.7736 | Val Loss: 0.2775 Acc: 0.7314 BalAcc: 0.7718 MacroF1: 0.7349 | LR: 0.000157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 022/60 | Train Loss: 0.3564 Acc: 0.7803 | Val Loss: 0.2341 Acc: 0.8382 BalAcc: 0.7743 MacroF1: 0.7689 | LR: 0.000153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 023/60 | Train Loss: 0.3637 Acc: 0.7702 | Val Loss: 0.2094 Acc: 0.8667 BalAcc: 0.7818 MacroF1: 0.7860 | LR: 0.000148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 024/60 | Train Loss: 0.3541 Acc: 0.7732 | Val Loss: 0.1957 Acc: 0.8695 BalAcc: 0.7353 MacroF1: 0.7748 | LR: 0.000143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 025/60 | Train Loss: 0.3364 Acc: 0.7894 | Val Loss: 0.2008 Acc: 0.8379 BalAcc: 0.7691 MacroF1: 0.7718 | LR: 0.000138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 026/60 | Train Loss: 0.3434 Acc: 0.7800 | Val Loss: 0.2099 Acc: 0.8633 BalAcc: 0.7493 MacroF1: 0.7821 | LR: 0.000133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 027/60 | Train Loss: 0.3284 Acc: 0.7792 | Val Loss: 0.2041 Acc: 0.8664 BalAcc: 0.7697 MacroF1: 0.7894 | LR: 0.000128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 028/60 | Train Loss: 0.3225 Acc: 0.7875 | Val Loss: 0.2588 Acc: 0.8482 BalAcc: 0.7630 MacroF1: 0.7684 | LR: 0.000123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 029/60 | Train Loss: 0.2982 Acc: 0.8026 | Val Loss: 0.2104 Acc: 0.8598 BalAcc: 0.7977 MacroF1: 0.7968 | LR: 0.000118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/60 [Train]:   5%|▌         | 37/679 [00:05<01:35,  6.73it/s, acc=0.7860, loss=0.3461]"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# INSTALLS\n",
    "# =============================\n",
    "!pip install --quiet timm scikit-learn albumentations==1.4.10 opencv-python\n",
    "\n",
    "# =============================\n",
    "# IMPORTS\n",
    "# =============================\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import timm\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, balanced_accuracy_score, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# =============================\n",
    "# CONFIG\n",
    "# =============================\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_DIR = \"/home/dineshkumarv.22it/preprocessed_data\"\n",
    "IMG_DIR = os.path.join(DATA_DIR, \"images\")\n",
    "ARR_DIR = os.path.join(DATA_DIR, \"arrays\")\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS = 60\n",
    "IMG_SIZE = 224\n",
    "LR = 2e-4\n",
    "WD = 1e-4\n",
    "PATIENCE = 10\n",
    "MIXUP_ALPHA = 0.4\n",
    "CUTMIX_ALPHA = 0.4\n",
    "P_MIXUP = 0.5\n",
    "P_CUTMIX = 0.5\n",
    "SEED = 42\n",
    "FREEZE_WARMUP_EPOCHS = 3\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "set_seed()\n",
    "\n",
    "# =============================\n",
    "# LOAD ARRAYS\n",
    "# =============================\n",
    "image_ids = np.load(os.path.join(ARR_DIR, \"image_ids.npy\"), allow_pickle=True)\n",
    "labels = np.load(os.path.join(ARR_DIR, \"y_labels.npy\"), allow_pickle=True)\n",
    "class_names = np.load(os.path.join(ARR_DIR, \"label_classes.npy\"), allow_pickle=True)\n",
    "\n",
    "num_classes = int(len(class_names))\n",
    "print(\"Classes:\", class_names, \"\\n#classes:\", num_classes, \" #images:\", len(image_ids))\n",
    "\n",
    "# =============================\n",
    "# TRANSFORMS\n",
    "# =============================\n",
    "train_tf = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.3),\n",
    "    A.RandomRotate90(p=0.3),\n",
    "    A.Affine(scale=(0.9, 1.1), translate_percent=(0.05, 0.05), rotate=(-15, 15), p=0.5),\n",
    "    A.ColorJitter(p=0.3),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.CLAHE(p=0.2),\n",
    "    A.GaussianBlur(blur_limit=(3, 5), p=0.2),\n",
    "    A.CoarseDropout(\n",
    "        min_holes=1, max_holes=8,\n",
    "        min_height=8, max_height=16,\n",
    "        min_width=8, max_width=16,\n",
    "        fill_value=0, p=0.5\n",
    "    ),\n",
    "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "\n",
    "val_tf = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# =============================\n",
    "# DATASET\n",
    "# =============================\n",
    "class SkinDataset(Dataset):\n",
    "    def __init__(self, image_ids, labels, img_dir, transform=None):\n",
    "        self.image_ids = list(image_ids)\n",
    "        self.labels = list(labels)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        label = int(self.labels[idx])\n",
    "        img_path = os.path.join(self.img_dir, img_id + \".jpg\")\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception:\n",
    "            img = Image.new(\"RGB\", (IMG_SIZE, IMG_SIZE), (0, 0, 0))\n",
    "        img = np.array(img)\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)[\"image\"]\n",
    "        return img, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# =============================\n",
    "# SPLIT TRAIN/VAL\n",
    "# =============================\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_ids, val_ids, train_labels, val_labels = train_test_split(\n",
    "    image_ids, labels, test_size=0.3, stratify=labels, random_state=SEED\n",
    ")\n",
    "\n",
    "train_dataset = SkinDataset(train_ids, train_labels, IMG_DIR, transform=train_tf)\n",
    "val_dataset = SkinDataset(val_ids, val_labels, IMG_DIR, transform=val_tf)\n",
    "\n",
    "# =============================\n",
    "# HANDLE IMBALANCE\n",
    "# =============================\n",
    "class_counts = Counter(train_labels)\n",
    "class_weights = {cls: 1.0 / max(1, count) for cls, count in class_counts.items()}\n",
    "sample_weights = [class_weights[int(l)] for l in train_labels]\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# =============================\n",
    "# MIXUP / CUTMIX HELPERS\n",
    "# =============================\n",
    "def rand_bbox(W, H, lam):\n",
    "    cut_w = int(W * math.sqrt(1 - lam))\n",
    "    cut_h = int(H * math.sqrt(1 - lam))\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "    x1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    y1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    x2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    y2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def apply_mixup_cutmix(images, targets, mixup_alpha=MIXUP_ALPHA, cutmix_alpha=CUTMIX_ALPHA,\n",
    "                         p_mixup=P_MIXUP, p_cutmix=P_CUTMIX):\n",
    "    B = images.size(0)\n",
    "    if B < 2:\n",
    "        return images, targets, targets, 1.0, None\n",
    "    mode = None\n",
    "    r = random.random()\n",
    "    if mixup_alpha > 0 and r < p_mixup:\n",
    "        mode = 'mixup'\n",
    "        lam = np.random.beta(mixup_alpha, mixup_alpha)\n",
    "        index = torch.randperm(B, device=images.device)\n",
    "        mixed = lam * images + (1 - lam) * images[index, :]\n",
    "        return mixed, targets, targets[index], lam, mode\n",
    "    if cutmix_alpha > 0 and r >= p_mixup and r < p_mixup + p_cutmix:\n",
    "        mode = 'cutmix'\n",
    "        lam = np.random.beta(cutmix_alpha, cutmix_alpha)\n",
    "        index = torch.randperm(B, device=images.device)\n",
    "        _, H, W = images.size(1), images.size(2), images.size(3)\n",
    "        x1, y1, x2, y2 = rand_bbox(W, H, lam)\n",
    "        mixed = images.clone()\n",
    "        mixed[:, :, y1:y2, x1:x2] = images[index, :, y1:y2, x1:x2]\n",
    "        lam = 1 - ((x2 - x1) * (y2 - y1) / (W * H))\n",
    "        return mixed, targets, targets[index], lam, mode\n",
    "    return images, targets, targets, 1.0, None\n",
    "\n",
    "# =============================\n",
    "# FOCAL LOSS\n",
    "# =============================\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.ce = nn.CrossEntropyLoss(reduction='none')\n",
    "    def forward(self, inputs, targets):\n",
    "        ce = self.ce(inputs, targets)\n",
    "        pt = torch.exp(-ce)\n",
    "        loss = self.alpha * (1 - pt) ** self.gamma * ce\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        return loss\n",
    "\n",
    "def mix_criterion(criterion, outputs, y1, y2, lam, mode):\n",
    "    if mode in ('mixup', 'cutmix'):\n",
    "        return lam * criterion(outputs, y1) + (1 - lam) * criterion(outputs, y2)\n",
    "    else:\n",
    "        return criterion(outputs, y1)\n",
    "\n",
    "# =============================\n",
    "# MODEL\n",
    "# =============================\n",
    "class ConvNeXtSwin(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.convnext = timm.create_model(\"convnext_base\", pretrained=True, num_classes=0, global_pool=\"avg\")\n",
    "        self.swin = timm.create_model(\"swin_base_patch4_window7_224\", pretrained=True, num_classes=0, global_pool=\"avg\")\n",
    "        feat_dim = self.convnext.num_features + self.swin.num_features\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 768),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        f1 = self.convnext(x)\n",
    "        f2 = self.swin(x)\n",
    "        out = torch.cat([f1, f2], dim=1)\n",
    "        return self.head(out)\n",
    "\n",
    "model = ConvNeXtSwin(num_classes).to(DEVICE)\n",
    "\n",
    "def set_backbone_grad(enabled: bool):\n",
    "    for p in model.convnext.parameters():\n",
    "        p.requires_grad = enabled\n",
    "    for p in model.swin.parameters():\n",
    "        p.requires_grad = enabled\n",
    "set_backbone_grad(False)\n",
    "\n",
    "# =============================\n",
    "# OPTIMIZER + SCHEDULER\n",
    "# =============================\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR, weight_decay=WD)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=LR * 0.05)\n",
    "criterion = FocalLoss(alpha=1.0, gamma=2.0).to(DEVICE)\n",
    "\n",
    "# =============================\n",
    "# TRAINING LOOP\n",
    "# =============================\n",
    "best_val_f1 = -1.0\n",
    "best_state = None\n",
    "patience_counter = 0\n",
    "\n",
    "def evaluate(model, loader, device=DEVICE):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in loader:\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            lbls = lbls.to(device, non_blocking=True)\n",
    "            out = model(imgs)\n",
    "            loss = criterion(out, lbls)\n",
    "            val_loss += loss.item() * imgs.size(0)\n",
    "            preds = out.argmax(dim=1)\n",
    "            y_true.extend(lbls.cpu().numpy().tolist())\n",
    "            y_pred.extend(preds.cpu().numpy().tolist())\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    bacc = balanced_accuracy_score(y_true, y_pred)\n",
    "    mf1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return val_loss / len(loader.dataset), acc, bacc, mf1, y_true, y_pred\n",
    "\n",
    "scaler = torch.amp.GradScaler(\"cuda\", enabled=torch.cuda.is_available())\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    if epoch == FREEZE_WARMUP_EPOCHS + 1:\n",
    "        set_backbone_grad(True)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS - epoch + 1, eta_min=LR * 0.05)\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS} [Train]\", leave=False)\n",
    "    for imgs, lbls in pbar:\n",
    "        imgs = imgs.to(DEVICE, non_blocking=True)\n",
    "        lbls = lbls.to(DEVICE, non_blocking=True)\n",
    "        imgs_mixed, y1, y2, lam, mode = apply_mixup_cutmix(imgs, lbls)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.amp.autocast(\"cuda\", enabled=torch.cuda.is_available()):\n",
    "            outputs = model(imgs_mixed)\n",
    "            loss = mix_criterion(criterion, outputs, y1, y2, lam, mode)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        \n",
    "        # Calculate correct predictions based on original labels for accuracy logging\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        if mode in ('mixup', 'cutmix'):\n",
    "            correct += (preds == y1).sum().item() * lam + (preds == y2).sum().item() * (1 - lam)\n",
    "        else:\n",
    "            correct += (preds == lbls).sum().item()\n",
    "\n",
    "        total += lbls.size(0)\n",
    "        pbar.set_postfix(loss=f\"{running_loss / total:.4f}\", acc=f\"{correct / total:.4f}\")\n",
    "    \n",
    "    scheduler.step()\n",
    "    val_loss, val_acc, val_bacc, val_mf1, y_true, y_pred = evaluate(model, val_loader)\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_acc = correct / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch:03d}/{EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} BalAcc: {val_bacc:.4f} MacroF1: {val_mf1:.4f} | \"\n",
    "          f\"LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    if val_mf1 > best_val_f1:\n",
    "        best_val_f1 = val_mf1\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}. Best Macro F1: {best_val_f1:.4f}\")\n",
    "            break\n",
    "\n",
    "# =============================\n",
    "# LOAD BEST & FINAL EVAL\n",
    "# =============================\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "val_loss, val_acc, val_bacc, val_mf1, y_true, y_pred = evaluate(model, val_loader)\n",
    "print(\"\\n=== FINAL VALIDATION ===\")\n",
    "print(f\"Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | Balanced Acc: {val_bacc:.4f} | Macro F1: {val_mf1:.4f}\")\n",
    "print(\"\\nPer-class F1:\")\n",
    "print({class_names[i]: f\"{f1:.3f}\" for i, f1 in enumerate(f1_score(y_true, y_pred, average=None))})\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[str(c) for c in class_names]))\n",
    "\n",
    "# =============================\n",
    "# CONFUSION MATRIX\n",
    "# =============================\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================\n",
    "# SAVE MODEL\n",
    "# =============================\n",
    "save_path = \"/home/dineshkumarv.22it/convnext_swin_focal_mix_aug_best.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(\"Model saved to:\", save_path)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
